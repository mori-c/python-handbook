{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Level Sets | Rosenbrock | Wiener Filter | Hyperbolic |\n",
    "| ---------- | ---------- | ------------- | ---------- |\n",
    "| ![gradient descent level sets](https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/350px-Gradient_descent.svg.png) | ![gradient descent rosenbrock function narrow curve value minimum](https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Banana-SteepDesc.gif/400px-Banana-SteepDesc.gif) <!-- \\| ![gradient descent rosenbrock function narrow curve value minimum](https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Gradient_ascent_%28contour%29.png/350px-Gradient_ascent_%28contour%29.png) --> | ![gradient descent steepest descent algorithm wiener filer](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Steepest_descent.png/380px-Steepest_descent.png) | ![gradient descent hyperbolic](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Gradient_ascent_%28surface%29.png/450px-Gradient_ascent_%28surface%29.png)\n",
    "\n",
    "# Gradient descent <sup>[01](#wiki-gradient)</sup>\n",
    "\n",
    "A first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. If, instead, one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\n",
    "\n",
    "Gradient descent is also known as steepest descent. However, gradient descent should not be confused with the method of steepest descent for approximating integrals.\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<a name=\"wiki-gradient\">01</a> : [Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent in Pure Python\n",
      "Algorithm Setup\n",
      "\n",
      "> parameters of w_0 and w_1 range, 0 as starter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# source: https://is.gd/TkvVCJ\n",
    "print('Gradient Descent in Pure Python')\n",
    "print('Algorithm Setup')\n",
    "print()\n",
    "print('> parameters of w_0 and w_1 range, 0 as starter')\n",
    "print()\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "# py_descent will contain x(dataset), d(descent), mu, N(dataset columns), epochs(training loss)\n",
    "def py_descent(x, d, mu, N_epochs):\n",
    "    N = len(x)\n",
    "    print(N)\n",
    "    f = 2 / N             # function(algm) = 2 / rows??\n",
    "    \n",
    "    # 'empty' = predictions, errors(err), weights(w_0), gradients(grad)\n",
    "    y = [0] * N           # y = columns\n",
    "    w = [0,0]             # weight parameters @ zero\n",
    "    grad = [0,0]          # minimum @ zero\n",
    "    print('y: ', y, 'w: ', w, 'grad: ', grad)\n",
    "    \n",
    "    for _ in it.repeat(None, N_epochs):\n",
    "        # must access elements in array twice, hence no generator\n",
    "        # error statement for debugging\n",
    "        err = tuple(i - j, j in zip(d, y))\n",
    "        grad[0] = f * sum(err)\n",
    "        grad[1] = f * sum(i * j for i, j in zip(err, x))\n",
    "        w = [i + mu * j for i, j in zip(w, grad)]         #\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
